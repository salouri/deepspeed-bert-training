DeepSpeed BERT Training

This repository contains the setup and scripts to train a BERT model using DeepSpeed.

Setup

Step 1: Clone the Repository

git clone https://github.com/salouri/deepspeed-bert-training.git
cd deepspeed-bert-training

Step 2: Create and Activate Conda Environment

conda create -n deepspeed python=3.10
conda activate deepspeed

Step 3: Install Dependencies

pip install -r requirements.txt

Step 4: Run the Script

To start training the BERT model using DeepSpeed, use the following command:

    deepspeed --num_gpus=0 --bind_cores_to_rank train_bert_ds.py

 Note: If you encounter any issues with numactl or don't require NUMA-specific optimizations, just bypass it by
 not using the --bind_cores_to_rank option. Simply run:
    deepspeed --num_gpus=0 train_bert_ds.py
Dependencies

- Python 3.10
- DeepSpeed
- mpi4py
- Other dependencies listed in requirements.txt
